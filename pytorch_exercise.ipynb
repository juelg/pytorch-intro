{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763f2742",
   "metadata": {},
   "source": [
    "# Exercise: MNIST\n",
    "by Tobias JÃ¼lg\n",
    "\n",
    "Disclaimer: This exercise is partly based on pytorch's [Quickstart Tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html).\n",
    "\n",
    "## Install PyTorch and PyTorch Lightning\n",
    "Have a look at the [pytorch's installation page](https://pytorch.org/get-started/locally/). If the CPU version is sufficient for you then running the following cell should be sufficient. Note that pytorch only works with python3.9 so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32172c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to install\n",
    "#!pip install numpy\n",
    "#!pip install torch\n",
    "#!pip install torchvision\n",
    "#!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec97ddd1",
   "metadata": {},
   "source": [
    "## Exercise 1: Plain PyTorch\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963ed245",
   "metadata": {},
   "source": [
    "### Data\n",
    "In this exercise we use the MNIST dataset which consists of 28x28 pictures of handwritten digits. The task is to classify the pictures into the numbers  0 to 9. PyTorch already has a Dataset implemented for MNIST so we dont have to perform this yourselfs. Conveniently, the dataset can also download the dataset for us. The following code does exactly that.\n",
    "\n",
    "1. Look up what **transforms** are in the [pytorch docu](https://pytorch.org/vision/stable/transforms.html) and especially check out what the `ToTensor()` transform does. Do we have to normalize our data before we put it into the network?\n",
    "2. Create two dataloaders for our train and validation datasets (TODO2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958c2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "val_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = #TODO2: add dataloaders\n",
    "val_dataloader = #TODO2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07bd93e",
   "metadata": {},
   "source": [
    "### Network\n",
    "3. Checkout the shapes of our training data. How must the input of your network look like to take such data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f3cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3: checkout shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a26a0",
   "metadata": {},
   "source": [
    "4. Complete the network's code using the following topology using the [`nn.Sequential` module](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) (TODO4):\n",
    "\n",
    "$$W_3 ReLU(W_2 ReLU(W_1 x + b_1) + b_2) + b_3$$\n",
    "\n",
    "where $W_1\\in\\mathbb{R}^{512\\times28\\cdot28}$, $b_1\\in\\mathbb{R}^{512}$,\n",
    "$W_2\\in\\mathbb{R}^{512\\times512}$, $b_2\\in\\mathbb{R}^{512}$,\n",
    "$W_3\\in\\mathbb{R}^{10\\times512}$, $b_3\\in\\mathbb{R}^{10}$\n",
    "\n",
    "What does the [`nn.Flatten()` layer](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208dcbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassicNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # TODO 4: add layers as described above\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b74898",
   "metadata": {},
   "source": [
    "We now want to create a second network which uses convolutions instead of fully connected layers. Convolutions can be seen as learnable filters. They are much better in detecting local patterns as the weight kernel \"slides\" over the image and uses the same weights over and over again. This means that they will also result in less trainable parameters. Thus, they are perfect feature extractors for images. [This article](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) will give you a good overview of how convolutions work if you are new to the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c4ce72",
   "metadata": {},
   "source": [
    "5. Look up how [`nn.Conv2d` layers](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) work. As the handwritten digits are just in grayscale, they only have one color channel, as you should know from question 3. So you should start with channel dimension 1 then go up to 16 in the first convolution, then to 32 in the second. Use a kernel size of 2, stride of 2 and padding of 1 for the convolutions. Use the `nn.ReLU` activation function in between the layers. Finally use a `nn.Flatten` layer and a fully connected (linear) layer to get the output down to our 10 output neurons.\n",
    "\n",
    "The following formula might come in handy to calculate the input size of the fully connected layer:\n",
    "\n",
    "$$H_{out} = \\lfloor\\frac{H_{in} + 2*padding - (kernel-1)-1}{stride}+1\\rfloor$$\n",
    "\n",
    "where $H_{in}$ is the channel dimension of the layer before and $H_{out}$ is the output dimension of the current layer. What is the output dimension of the second convolution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn_relu_stack = nn.Sequential(\n",
    "            # TODO 5: add layers as described above\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.cnn_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f3884",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "6. Which loss function should we use this kind of problem.\n",
    "7. Given that we want to use `nn.CrossEntropyLoss()` as loss. Do we need to add a softmax layer in the end? Look up the documentation of the loss function for your answer.\n",
    "8. The class label comes in the form of a single number {0, ..., 9}. Look up what a one-hot vector is. Given that we want to use `nn.CrossEntropyLoss()` as loss, do we need to convert our ground truth labels to one-hot vectors? A: No as `nn.CrossEntropyLoss()` also handles this and does not need one-hot vectors.\n",
    "9. Complete the loss function in the code snipped below (TODO 9)\n",
    "10. Compltete the train_loop by adding the forward pass, the loss function calculation and the optimizer code (TODO 10)\n",
    "11. Why is the val_loop function inefficient? Add according code to make it more efficient. (TODO 11)\n",
    "12. Test you code, you might also want to change the model to your CNN model from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a24f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "loss_fn = #TODO 9: add the loss function\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # TODO 10: Compute prediction and loss\n",
    "\n",
    "        # TODO 10: Backpropagation and optimizer step\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def val_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    val_loss, correct = 0, 0\n",
    "\n",
    "    # TODO 11: why is this code inefficient, what is missing here?\n",
    "    for X, y in dataloader:\n",
    "        pred = model(X)\n",
    "        val_loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Val Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")\n",
    "    \n",
    "    \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    val_loop(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097b7895",
   "metadata": {},
   "source": [
    "## Exercise 2: PyTorch Lightning\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be06d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8a5a6",
   "metadata": {},
   "source": [
    "1. Complete the missing validation step: After the forward pass and the loss function: Log the loss and the accuracy. You might need to use the `validation_epoch_end` method to calculate the validation accuracy after the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLModule(pl.LightningModule):\n",
    "    def __init__(self, model, data, hparms):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.crit = nn.CrossEntropyLoss()\n",
    "        self.hparams.update(hparams)\n",
    "        self.data = data\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch[0], batch[1]\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.crit(y_hat, y)\n",
    "        self.log(\"loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch[0], batch[1]\n",
    "        # TODO 1: forward pass, loss calcuation and loss logging\n",
    "    \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        # TODO 1: calculate accuracy from validation_step outputs\n",
    "        pass\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.data[0], batch_size=self.hparams[\"batch_size\"])\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.data[1], batch_size=self.hparams[\"batch_size\"])\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.model.parameters(),\n",
    "                                self.hparams[\"learning_rate\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eea7bbb",
   "metadata": {},
   "source": [
    "2. Create the `hparams` dictionary containing our `learning_rate` (0.001), the `batch_size` (64) and the amount of `epochs` that you want to train. This dictionary is passed to the pytorch lightning module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b22691",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = # TODO 2: add values in the hparams dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b117459",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassicNN()\n",
    "pl_module = PLModule(model, (training_data, val_data), hparams)\n",
    "trainer = pl.Trainer(\n",
    "    #callbacks=callbacks,\n",
    "    max_epochs=hparams[\"epochs\"],\n",
    "    deterministic=True,\n",
    "    #gpus=[0],\n",
    "    #profiler=\"simple\",\n",
    ")\n",
    "trainer.fit(pl_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b342d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the logged data in tensorboard:\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a525fcd0",
   "metadata": {},
   "source": [
    "Possible further steps to go down the rabbit hole:\n",
    "- Train more epochs\n",
    "- Play around with the two differnt models. Extend the models with your own ideas.\n",
    "- Add early stopping and model checkpoints\n",
    "- If you have a GPU, try out GPU training\n",
    "- Look up the [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) layer and add it to your networks. How can batchnorm help us in having a more stable training?\n",
    "- Research how we can use [TorchMetrics](https://torchmetrics.readthedocs.io/en/stable/pages/lightning.html) to simpify metric logging\n",
    "- Replace SGD with the Adam optimizer and add weight decay to combat overfitting\n",
    "- profile the training to find out how much faster GPU training is. What takes the most time in your training?\n",
    "- Load the data with `num_workers=6` to fully utilze your CPU\n",
    "- Visualize the wrongly classified images. Why do you think they are wrongly classified?\n",
    "- Use a different dataset e.g. FashionMNIST. Do you achive a better accuracy? Why could there be a difference?\n",
    "- Look up transfer learning. How does pytorch support transfer learning? Use a pretrained VGG net\n",
    "- Look up ResNet. Explain the concept of skip connections and why they are usful when training very deep neural networks. Implement a skip connection layer yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31929ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
